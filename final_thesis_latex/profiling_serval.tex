\newpage
\subsection{Profiling the Serval prototype implementation}
Among the admirable headliners of Serval is the working prototype version of the proposed architecture.
In more than 28000 lines of code covering functionality of the Service Access Layer (both in userlevel operation and as a Linux kernel module), bindings for multiple programming languages, a translator, libraries and examples for writing Serval compatible applications, and with a reported throughput comparable to the unmodified TCP/IP stack, it is clearly showcased the feasibility of the solution.



In this section we are profiling the prototype in regard to the following parameters:
\begin{enumerate} \itemsep1pt \parskip0pt 
\parsep0pt
  \item CPU Instructions and Cycles
  \item System Call execution times
  \item Execution time needed for the completion of a numbered iteration of requests
  \item Number of packers per request, bytes on the wire
\end{enumerate}
Then we will be presenting the results juxtaposed to the measurements of the unchanged TCP/IP stack and the AF\_INET family.

\paragraph{} Output was obtained on a HP Prodesk 600 G1 --Intel(R) Core(TM) i5-4570 @ 3.20GHZ, 4GB RAM-- machine running Ubuntu 11.04 (Natty Narwahl) kernel version 2.6.38-16-generic (rebuilt with debug symbols).

The profiling tools we used include gprof, perf, oprofile, valgrind, strace, zoom and google performance tools.
For the tests with gprof, valgrind, oprofile and strace, Serval module and http\_client were built with debug symbols.
Extra, for gprof testing, Serval was built with CFLAGS, LDFLAGS and CPPFLAGS equal to "-pg".

For the measurements we ported libmicrohttpd to use Serval active sockets.
Also, we implemented a simple HTTP client which supports both AF\_INET and the AF\_SERVAL socket families, depending on the options passed during the call.
For each case, we used the INET and Serval version of librehttpd and http\_client respectively.
Therefore, besides the connectivity parts, both INET and SERVAL versions are working with the same logic in creating, exchanging and processing requests.
This way we believe the tests can give unbiased results, which would not be the case if for example we used Apache server for INET and libmicrohttpd for SERVAL requests.

Source code of libmicrohttpd and http\_client, along with the integration of Serval patch in the build procedure of nginx, can be found in the Appendix (\ref{sec:appendix}).
Benchmark results are published in the ServalDHT repository \footnote{\url{https://github.com/misaakidis/ServalDHT}}.



\subsubsection{Serval's performance in the worst case scenario}
In a network architecture benchmarking what matters the most is its total throughput under a stress test.
In other words, the data rate of information (both headers and payload) and the number of packets that can be processed during a specific time frame.
An excellent tool for this case, \emph{iperf}, proved Serval's TCP throughput to be very close to the original TCP's one, almost fully utilizing a GigE interface. \nomenclature{GigE}{Gigabit Ethernet}
The authors explain that the existing small difference is due to missing optimizations in Serval's prototype.

We are examining Serval from a completely different perspective.
We dive into the implementation of SAL and serval.ko module and the overhead in using the Serval APIs.
And we do so in the worst possible scenario for an architecture that is establishing its own identifiers in the end host stacks.

It is SAL's responsibility to create the flows and synchronize the sockets in either side \footnote{Specifically for TCP, Serval is using the functionality that corresponds only to the ESTABLISHED state.}.
This means that when binding an active socket to a serviceID, the SAL must insert an entry into the flow table, register the service in the service table with a DEMUX rule and propagate the service registration to the network (may it be an anycast flood or a request to a singe service router).
Also, when connecting to a service, SAL must first convert a service name to a serviceID, resolve the serviceID, and finally establish a connection using CONTROL and SERVICE headers.
Correspondingly, closing a connection requires the exchange of packets with CONTROL headers.
In both cases, must-have checks like whether a serviceID is of an appropriate format, are more computational effort to the perquisite ones.

Once a connection has been established, the only significant overhead is the addition of a 12 bytes Serval header with the source and destination flowIDs, and the demultiplexing of incoming packets.
We can presume for those reasons, that Serval (and any other relative architecture) is struggling during the process of establishing a connection.

The case scenario we are testing is consisted of a client that connects to a service and requests information that can fit within a single response packet.
Since both the server and the client are running in the same machine, we can presume that the available bandwidth exempts network "links" from being responsible for a bottleneck.
The results show how well the Serval implementation can manage when it is pushed to the limit.



\newpage
\paragraph{Instructions and CPU Cycles} \hfill \\
Using perf performance counters subsystem in Linux \footnote{\url{https://perf.wiki.kernel.org/}}, we were able to benchmark the http\_client application with PF\_INET and PF\_SERVAL socket protocol families in a CPU cycle level.
There is definitely an increase in the Instructions and CPU cycles of the PF\_SERVAL version, as presented in Table \ref{table:cpu}, but taking a closer look in the perf output one will notice that there is a great increase in cache-misses and in the amount of branches.
Therefore the Instructions increase is a less biased index of the complexity added in the Serval port.
\begin{table}
\begin{center}
  \begin{tabular}{l||cc|cc|c}
  	\toprule
  	Metric			&	\multicolumn{2}{c}{PF\_INET}	&	\multicolumn{2}{c}{PF\_SERVAL}	&	Increase	\\
  	\midrule
    Instructions	&	690,559		&	$\pm$0.009\%	&	1,864,287	&	$\pm$0.054\%	&	169.9\%		\\
    CPU Cycles		&	1,019,096	&	$\pm$0.062\%	&	8,799,912	&	$\pm$0.191\%	&	763.5\%		\\
    Cache-misses	&	685			&	$\pm$1.417\%	&	72,893		&	$\pm$0.217\%	&	10541.3\%		\\
    Branches		&	125,548		&	$\pm$0.009\%	&	348,329		&	$\pm$0.057\%	&	177.4\%		\\
    \bottomrule
  \end{tabular}
  \caption[Benchmark: Instructions and CPU Cycles]{Instructions and CPU Cycles in 10000 runs of http\_client}
  \label{table:cpu}
\end{center}
\end{table}



\paragraph{System Calls execution times} \hfill \\
The results in Table \ref{table:stat} from stat user call \footnote{\url{http://man7.org/linux/man-pages/man1/stat.1.html}} show the independence of the time system calls need to complete, regardless the use of INET or SERVAL protocol family sockets.
This was expected and validated, since SAL in the kernel level is using the same system calls for its networking functions.
\textbf{\begin{table}
\begin{center}
  \begin{tabular}{l||c|c}
  	\toprule
  	Syscall			&	PF\_INET	&	PF\_SERVAL	\\
  	\midrule
    socket			&	0.145		&	0.070		\\
    setsockopt		&	0.160		&	0.118		\\
    connect			&	0.117		&	0.113		\\
    getsockname		&	0.300		&	0.552		\\
    send			&	0.089		&	0.086		\\
    recv			&	0.159		&	0.313		\\
    close			&	0.086		&	0.087		\\
    \bottomrule
  \end{tabular}
  \caption[Benchmark: System Call execution times]{Kernel System Calls execution times (in milliseconds)}
  \label{table:stat}
\end{center}
\end{table}}



\newpage
\paragraph{Finite Requests loop timing} \hfill \\
In this test we calculate --again using perf-- the milliseconds needed for the execution of a single request.
It is noteworthy that after a number of requests the execution time fails significantly.
Results listed in Table \ref{table:reqtime}
\begin{table}
\begin{center}
  \begin{tabular}{l||cc|cc|c}
  	\toprule
  	Requests	&	\multicolumn{2}{c}{PF\_INET}	&	\multicolumn{2}{c}{PF\_SERVAL}	&	Increase	\\
  	\midrule
    10			&	1.171		&	$\pm$2.171\%	&	1.845		&	$\pm$1.542\%	&	57.5\%		\\
    100			&	0.399		&	$\pm$7.248\%	&	0.686		&	$\pm$9.404\%	&	71.9\%		\\
    1000		&	0.590		&	$\pm$5.835\%	&	0.837		&	$\pm$7.751\%	&	41.9\%		\\
    \bottomrule
  \end{tabular}
  \caption[Benchmark: Requests execution times]{Requests execution times}
  \label{table:reqtime}
\end{center}
\end{table}



\paragraph{Packets specific metrics} \hfill \\
After all, we are focusing on the packets sent for each HTTP request.
Serval again has to send 5 more packets with control headers for the establishment and closing of the connection.
Finally there is an increase in the bytes transferred on the wire.
It is wise to highlight at this point that in a case where the payload would need to be split in more packets, or the conenction to be used as a stream for communication, that the observed overhead of extra packets would be negligible, and only the 12-byte header of serval would add bits on the stream.
\begin{table}
\begin{center}
  \begin{tabular}{l||c|c}
  	\toprule
  	Metric				&	PF\_INET	&	PF\_SERVAL	\\
  	\midrule
    Packets/request		&	10			&	15			\\
    Bytes/request		&	922			&	1392		\\
    \bottomrule
  \end{tabular}
  \caption[Benchmark: Packets and bytes exchanged per request]{Packets and bytes exchanged per request}
  \label{table:packets}
\end{center}
\end{table}



\newpage
\subsubsection{Benchmark results files}
As a reference, you can find the results of performance testing in the public repository, under the \emph{benchmarking} directory.
Specifically for INET and SERVAL you will find:
\begin{enumerate}
  \item \emph{gprof\_httpclient}: Results from gprof profiling tool. One can see the call graph of http\_client. Execution is completed fast enough to show zero accumulated time with the 0.01 seconds samples.
  \item \emph{kcachegrindgui\_httpclient}: Annotated source code with valgrind's kcachegrind tool.
  \item \emph{kcachegrind\_httpclient}: Graphic representation of the call tree along with the Instructions of each function.
  \item \emph{libmicrohttpd\_wireshark.pcap}: Wireshark capture of INET and SERVAL sockets with the libmicrohttpd server.
  \item \emph{nginx\_wireshark.pcap}: Wireshark capture of INET and SERVAL sockets with the nginx server.
  \item \emph{opannotate\_httpclient}: Percentage of time spent on each file called (including shared libraries). Vmlinux corresponds to kernel space processes.
  \item \emph{perf\_httpclient}: Performance counter stats 10,000 requests.
  \item \emph{perf\_serval\_mod}: Map details of the serval dynamically shared object (dso). DSOs are the "gateway" between the user and kernel space, and act as the contact point when an application running in the user space makes a system call.
  \nomenclature{DSO}{Dynamically Shared Object}
  \item \emph{statc\_httpclient}: List of system calls.
  \item \emph{stat\_httpclient}: Ordered system calls with their execution time.
\end{enumerate}



\newpage
\subsubsection{Closing remarks on benchmark results}
Results from performance testing might be alerting.
As closing remarks of the benchmarking section, we would like to shape our thoughts on the topic.

Serval is a great, well-thought service-centric network architecture.
Its simple, genuine abstractions, its clean separation of the control and the data plane, its resilience in migration, its familiar APIs, all add upon a networking model which provides apparent benefits to service providers and users as well.

However, services have some congenital characteristics that are not met on all networked applications.
Above all, services rely on resolute connections.
Once the path is established, a flow is expected to retain its reachability and serve as the channel for constant communication with the other side.
A channel which ordinarily serves as the middle for large files exchange.
Thereupon, SYN packets are only a small proportion of the network traffic of services.
A small overhead thus in the connection establishment is acceptable, especially if it can prevent a future reinitiation of the same process.

In contrast, there are applications which are working in a completely different way.
Programs that wake up one in a day to send a single packet to a remote server.
Real time systems that have to deliver a small part of information to many recipients.
Embedded machines that produce burst data and which do not require every single packet to be delivered.

Those scenarios are far from rare.
VoIP, \nomenclature{VoIP}{Voice over IP} media streaming, online games, even the DNS itself are illustrative examples.
For many years now we have been treating them in a special way; connectionless communication.
UDP over TCP has been serving well in those situations that we want to send fast, limited amount of data or when we do not care about if the packets get delivered.

We understand that the tests conducted do not represent a common use case.
They indicate though a possible shortcoming of Serval and demonstrate an expected performance in the worst scenarios as described above.

In the long run, Serval should not be considered a nostrum.
As every networking paradigm, it servers best where it was originally intentioned to do so.
SAL's symbiotic capability with the TCP/IP stack gives us the final choice to reason in which circumstances to use the Serval APIs and when to follow the well-trodden path.